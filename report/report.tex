\documentclass[11pt]{article}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}


\title{Unidirectional recipe search using custom NER models}
\date{}
\author{Anton Gefvert \\ Link√∂ping University}

\begin{document}
\maketitle
\begin{abstract}
    lorem ipsum
\end{abstract}

\section{Introduction}
\section{Introduction}
Recently, there has been a significant increase of applications leveraging LLM
agents to perform diverse tasks.
Many of these applications make use of Retrieval Augmented Generation (RAG),
which involves dynamically loading data into an LLM query without explicit user
command or visibility.
This enables LLMs to process more accurate and specific results.
In this work, this concept is applied specifically to recipes, with the aim of
creating a system to search through recipes using ingredients.
This system can then be leveraged by an LLM agent to perform queries such as
returning recipes or serving as a reference when creating new ones.

Ingredient lists for recipes possess characteristics that are particularly
relevant for developing a more nuanced search algorithm.
Among these, preparations and variants are of specific interest.
Preparations refer to how food is prepared (e.g., ``2 Tomatoes, \emph{chopped
into cubes}'' or ``3 red onions \emph{sliced thinly}''), while variants denote
specific ingredient variations (e.g., ``3 \emph{Roma} tomatoes'' or ``1 large
\emph{granny smith} apple'').

Typically, when searching for recipes, a user would most likely input general
terms such as ``tomatoes'' or ``apples'' to find recipes containing those
ingredients.
However, in scenarios where a user has, for instance, excess sliced red onions
and seeks a specific recipe for their utilization, or harbors a preference for
the tartness of Granny Smith apples and desires tailored inspiration, a more
sophisticated search capability becomes desirable.
Nevertheless, forcing the search query to be overly specific (e.g., for ``sliced
red onions'') is often counterproductive, as general ingredients (like red
onions) can be prepared as needed.
Similarly, for general ingredient searches (e.g., apples), recipes are likely to
remain satisfactory even if a specific variant (like Granny Smith) is not
precisely matched.

This work operationalizes this concept by utilizing ingredient-list-specific
Named Entity Recognition (NER) tags to establish uni-directional relationships
between ingredients and their associated preparations and variants.
This approach aims to demonstrate how fine-grained semantic understanding,
enabled by NER, can significantly enhance recipe retrieval systems, paving the
way for more intuitive and flexible culinary exploration tools.

\section{Theory}
To understand all parts of this report, some theory for things not discussed in
the TDDE16 course will be briefly explained.
\subsection{Top-K Accuracy}
Top-K Accuracy is an evaluation method for ranking algorithms.
Top-K Accuracy looks at, for all your ranking queries, in how many of them is the
target in the Top-K results.
The formula would be $\frac{\text{queries where target in top K}}{\text{number of
queries}}$, giving a fraction between $0$ and $1$\cite{topkacc}.
\subsection{Mean Reciprocal Rank}
Reciprocal Rank describes how well a relevant document is ranked in a list of
documents, quantified as a value between $0$ and $1$.
Reciprocal Rank is calculated using the formula $RR = 1/rank$ where $rank$ is the
rank in the list of documents, so if the relevant document is found at position
$5$, the reciprocal rank would be $1/5 = 0.2$.

Mean Reciprocal Rank (MRR) is the mean of the reciprocal rank over all queries,
calculated using the formula $MRR = \frac{1}{|Q|} \sum_{q=1}^Q{RR_q}$ where $Q$ is
the set of all queries and $RR_q$ is the reciprocal rank for query $q \in
Q$~\cite{mrr}.
\subsection{Best Match 25}
Best Match 25 (BM25) is a probabilistic ranking function widely used in
information retrieval to determine the relevance of documents to a given query.
It builds upon the TF-IDF (Term Frequency-Inverse Document Frequency) model by
introducing two key improvements: term frequency saturation (where repeated
occurrences of a term contribute less to relevance over time) and document
length normalization. These enhancements, controlled by adjustable parameters,
allow BM25 to more accurately rank documents than simpler TF-IDF
approaches~\cite{ogBM25, improvmenetsBM25}.

\section{Data}
In this work, two premade datasets are used,
\emph{RecipeNLG}\footnote{\url{https://www.kaggle.com/datasets/paultimothymooney/recipenlg}}
containing over 2 million recipes, as well as
\emph{TASTEset}\footnote{\url{https://github.com/taisti/TASTEset}}, a dataset of
700 recipes with ingredient-specific NER tags.

A third dataset, made by relabeling some of the NER tags in the TASTEset dataset,
as well as labeling some of the data in the RecipeNLG using the NER tags
described in Section~\ref{sec:ner_tags} was also created by manually labeling
data in Label Studio~\cite{LabelStudio}.

\section{Method}
To create and evaluate this uni-directional relationship based retrieval system, 

\subsection{Evaluation Tasks}
To see if the developed models performs the task of recipe retrieval from
ingredients, some evaluation tasks are defined.
Two types of task is used in this paper, model performance and query speed.
The first one to evaluate if the developed method works well, specifically in
relation the the baselines.
The second one to evaluate the speed of which these queries are handled, to
gauge how suitable they are for different systems.
For example, in a load heavy system that require low response times, model
performance might be compromisable as long as the query speed remains low.

\subsubsection{Model Performance}\label{sec:modelperfomance}
To evaluate the model performance, four different evaluation methods were
created.
All the tasks are evaluated using \emph{Accuray},
\emph{Mean Reciprocal Rank (MRR)}, \emph{Recall@5}, to give a fair evaluation to
a search/ranking task.

\paragraph{Identity Query} --- This tasks takes x amounts of random recipes
and for each recipe, the ingredient list is inputted and the recipe should be
returned.
This task is used as a sanity check, as this tasks should in theory create the
same embeddings for any embedding models, such as the TF-IDF baseline, and thus
give those models 100\% accuracy.

\paragraph{LLM Processed Queries} --- This task uses 1000 random
recipes, and for each recipe, inputs the ingredient list to a LLM with several
prompts to rephrase the input.
This is used to mimic a more human like input, while leveraging the power of
LLMs to quickly generate a larger dataset than e human annotated one.
For each recipe, the following prompts are used, and each promp is then used as
its own evaluation task:
\begin{itemize}
    \item \textbf{``Summarize this ingredient list into a short query a person might type
        into google''} --- This returns a more human-esque way of inputting a
        recipe, rather than a list of ingredients and instructions.
        Since text like this query, is supposed to be input into an LLM that
        then uses a processed input to a function, it was also asked to process
        this sentence into a more structure format containing a title and a list
        of ingredients.
    \item \textbf{``Extract only the main food items and their preparations from
        this list''} --- This returns a list that is more akin to the query a
        person (or program) is more likely to input to the model.
    \item \textbf{``I have the following ingredients and some of their
            preparations. List the most important ingredients and their
        preparations I should use to search a recipe with''} --- This returns a
        list that is compromised only of the most important elements of the
        recipe.
\end{itemize}

\paragraph{Human Processed Queries} --- This task uses 50 random recipes, that
are a subset of the previous task, and for each recipe, the author processed
these list into a more human like input.
For each list of ingredients, two different processing methods were used as
described below:
\begin{itemize}
    \item \textbf{A list of ingredients} --- for example ``Cucumber,
        sun-dried tomatoes, onion and ground beef'', mostly without preparations
        unless they are too core to the recipe, e.g.\ ``ground'' beef.
        This is to mimic a sample input of ``I have X food, what can I cook with it''
    \item \textbf{A list of prepared key ingredients} --- in this list the core
        and most unusal ingredients, with all of their preparations are compiled
        into a list; for example ``drained
        and quartered artichoke hearts; smoked turkey breast, cubed''.
        This is used to see if filtering on particular preparations can find
        very specific recipies.
\end{itemize}o

\paragraph{Negated Queries} --- For this task, the original ingredient data is
transformed destructively to create a task where the objective is to \emph{not}
be able to find the original recipe when inputting the transformed data.
Since the task is inverted, so are the evaluation metrics, i.e.\ we use
$1-\text{MRR}$ and $1-\text{P@5}$ to evaluate these tasks.

The data is transformed in the following ways:
\begin{itemize}
    \item \textbf{Randomly add preparation steps} --- Random preparation steps
        are appended to $40\%$ (minimum 1) of the lines in the ingredient list.
        This tests how well we filter away recipes that does not contain the
        preparations we have performed.
    \item \textbf{something} --- something
\end{itemize}


\subsubsection{Query Speed}
Another important part of information retrieval systems is query speed,
therefore query speeds are also measured on all the tasks described in
Section~\ref{sec:modelperfomance}.

\subsection{Baselines}
\subsubsection{BOW (tfidf)}

\subsubsection{SentenceBERT embeddings}

\subsection{NER based parquet + duckdb database with custom scoring function}

\subsubsection{NER Tags}\label{sec:ner_tags}

\subsection{Ranking}
We need some way to perform ranking of the different models.
When using embeddings ranking gets trivial because we will use similarity
measures as our mean of searching either way.
A problem arise when we want to use normal database queries, as there is no way
to rank based of queries.
We can ofcourse sort by something like recipe rating, giving us the highest
rated recipes rather than the most relevant recipes (highest rated might be most
relevant).

An idea for how to rank the standar query version, is to use a score, similar to
tfidf for how many of the ingredients that is in the recipe, is in the query,
and negative score for any ingridient in the recipe that is not in the query.
For example if you have something like ``chopped tomato, parsley and tomato
paste'' as the query, you would add to the score for tomato, parsley and tomato
paste and any other ingredient would reduce the score.
The way something similar to tfidf can be used, is to decide how much the score
is affected by an ingredient, something like salt and pepper should not affect
the score too much, as these are prevalent in a lot of recipes and most likely
not something you would add to your query at all.
Using this, we could also assign scores to preparation steps, but this might be
redundant, as we will remove any recipe which does not contain the preparation
present in our query, and the point is you should be able to perform the
preparations if it is not already prepared.
In the above example, we would not find any recipes containing non-chopped
tomatoes, and if they are cooked as well, we can just cook our chopped tomatoes.

This does however raise the question, is a tfidf that also make use of our NER
tags to filter out results we do not want to see, sufficient? Maybe even more
efficient?
I also want to create a model using tfidf and filtering methods.
\subsection{NER Model}
\subsubsection{Labeling}

\section{Results}
\section{Discussion}
\section{Conclusion}

\bibliography{references}


\end{document}

