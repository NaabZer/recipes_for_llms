\documentclass[11pt]{article}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}


\title{Unidirectional recipe search using custom NER models}
\date{}
\author{Anton Gefvert \\ Linköping University}

\begin{document}
\maketitle
\begin{abstract}
    lorem ipsum
\end{abstract}
\section{Introduction}
Recent advancements in LLM agents, often leveraging Retrieval Augmented
Generation (RAG)~\cite{RAG}, enable more accurate and specific data processing
for diverse tasks.
This work applies these principles to recipe search, aiming to create a system
that allows LLM agents to find and reference recipes based on their
ingredients.

While conventional recipe searches typically focus on broad ingredient matching
(e.g., ``tomatoes'') they often overlook the nuanced needs stemming from
specific ingredient preparations (e.g., ``chopped into cubes'') or varietals
(e.g., ``Roma'' tomatoes).
A user might desire recipes specifically for ``sliced red onions'' to utilize
excess ingredients, or seek inspiration for ``Granny Smith'' apples due to a
particular preference.
However, an effective search must also understand that a general ingredient
(e.g., ``red onions'') can be prepared as needed, and a specific variant (e.g.,
``Granny Smith'') may not be strictly essential for a recipe.
This dynamic relationship, where a specific ingredient implies its general form,
presents a key challenge for traditional retrieval systems.

This work addresses this challenge by utilizing ingredient-list-specific Named
Entity Recognition (NER) tags to establish uni-directional relationships
between ingredients, their preparations, and variants.
This fine-grained semantic understanding, enabled by NER, significantly
enhances recipe retrieval systems, fostering more intuitive and flexible
culinary exploration tools.

\section{Theory}
To understand all parts of this report, some theory for things not discussed in
the TDDE16 course will be briefly explained.
\subsection{Top-K Accuracy}
Top-K Accuracy is an evaluation method for ranking algorithms.
Top-K Accuracy looks at, for all your ranking queries, in how many of them is the
target in the Top-K results.
The formula would be $\frac{\text{queries where target in top K}}{\text{number of
queries}}$, giving a fraction between $0$ and $1$\cite{topkacc}.
\subsection{Mean Reciprocal Rank}
Reciprocal Rank describes how well a relevant document is ranked in a list of
documents, quantified as a value between $0$ and $1$.
Reciprocal Rank is calculated using the formula $RR = 1/rank$ where $rank$ is the
rank in the list of documents, so if the relevant document is found at position
$5$, the reciprocal rank would be $1/5 = 0.2$.

Mean Reciprocal Rank (MRR) is the mean of the reciprocal rank over all queries,
calculated using the formula $MRR = \frac{1}{|Q|} \sum_{q=1}^Q{RR_q}$ where $Q$ is
the set of all queries and $RR_q$ is the reciprocal rank for query $q \in
Q$~\cite{mrr}.
\subsection{Best Match 25}
Best Match 25 (BM25) is a probabilistic ranking function widely used in
information retrieval to determine the relevance of documents to a given query.
It builds upon the TF-IDF (Term Frequency-Inverse Document Frequency) model by
introducing two key improvements: term frequency saturation (where repeated
occurrences of a term contribute less to relevance over time) and document
length normalization. These enhancements, controlled by adjustable parameters,
allow BM25 to more accurately rank documents than simpler TF-IDF
approaches~\cite{ogBM25, improvmenetsBM25}.

\section{Data}
In this work, two premade datasets are used,
\emph{RecipeNLG}\footnote{\url{https://www.kaggle.com/datasets/paultimothymooney/recipenlg}}
containing over 2 million recipes, as well as
\emph{TASTEset}~\cite{TASTEset}, a dataset of
700 recipes with ingredient-specific NER tags.

A third dataset, made by relabeling some of the NER tags in the TASTEset dataset,
as well as labeling 150 recipes from the RecipeNLG using the NER tags
described in Section~\ref{sec:ner_tags}, was created by manually labeling
data in Label Studio~\cite{LabelStudio}.
This dataset contains 850 recipes, and X tokens and Y NER tags.

\section{Method}
To create and evaluate this uni-directional relationship based retrieval system, 

\subsection{Evaluation Tasks}
Evaluation tasks are defined to assess baseline and developed model performance
in recipe retrieval, including unidirectional relationship filtering.
Two primary task types are used: model performance, evaluating retrieval
effectiveness, and query speed, gauging suitability for different systems based
on query handling time.

\subsubsection{Model Performance}\label{sec:modelperfomance}
Model performance was evaluated using nine distinct methods, primarily assessed
by Mean Reciprocal Rank (MRR) and Top-K Accuracy.
Queries were categorized into two groups: subset and adversarial.
Evaluation utilized two query datasets: 50 recipes with four hand-crafted
queries, and 1000 recipes with four computer-generated queries.
These were tested against three target datasets (1k, 10k, 100k recipes), all
\emph{RecipeNLG} subsets that ensured query recipe presence.
Full dataset testing was omitted due to time and hardware constraints.
\paragraph{Subset Queries}
Subset queries evaluate how well recipes are found using queries containing less information than the full recipe.
These include an identity query, three LLM-generated queries, and two hand-crafted queries.

\emph{Identity Query} --- This query directly uses the original recipe as input, serving as a sanity check for near-perfect results.

\emph{LLM Processed Queries} --- Using 1000 random recipes, ingredient lists
were rephrased by an LLM to mimic human input and rapidly generate a large query
dataset.
Each rephrasing prompt formed its own evaluation task: first, a
``Summarize\ldots'' prompt generated a human-esque natural language query,
subsequently structured into title and ingredient list formats; second, an
``Extract only the main food items\ldots'' prompt produced a focused list of
core ingredients and their preparations; and third, an ``I have the following
ingredients\ldots List the most important\ldots'' prompt created a list of only
the most crucial recipe elements.  

For full prompts, see Appendix~\ref{app:llm}. For final data, see the publicly
available associated git
repository.\footnote{\url{https://github.com/NaabZer/recipes_for_llms/blob/main/data/eval_data/processed_data.json}}

\emph{Human Processed Queries} --- For 50 recipes (a subset of the LLM task's
recipes), the author manually created more human-like inputs using two methods:
a general list of ingredients, typically without preparations unless core (e.g.,
``ground'' beef), designed to mimic inputs like ``What can I cook with X
food''; and a list of prepared key ingredients, focusing on core and unusual
components with all their preparations (e.g., ``drained and quartered artichoke
hearts''), intended to test retrieval of very specific recipes.

For human annotated data, see the publicly available associated git
repository.\footnote{\url{https://github.com/NaabZer/recipes_for_llms/blob/main/data/eval_data/human_annotations.json}} 

\paragraph{Adversarial Queries}
To evaluate unidirectional filtering, three adversarial query tasks were created.
These queries mimic inputs containing more specific information than the
original recipe, aiming \emph{not} to retrieve the original.
Consequently, metrics are inverted: $1-MRR$ and $1-\text{Top-K Accuracy}$.
These tasks comprise one automatically generated query and two hand-crafted queries.

\emph{Automatically added preparation suffixes} --- This query type appends
common preparation suffixes (e.g., ``, chopped'', ``, sliced'') to 40\% (minimum
1) of random ingredients in the 1000-query dataset.
This tests the model's ability to filter out recipes lacking the added preparations.

\emph{Human annotated adversarial data} --- These two queries involve adding
variants or preparations to roughly 40\% of the human-annotated ingredient list
data that directly oppose the original (e.g., ``roma tomatoes, chopped''
becoming ``cherry tomatoes, sliced'').
One dataset includes all ingredients, while the other contains only the modified
ingredients.

\subsubsection{Query Speed}
Query speed is a critical metric for assessing the practical viability of
information retrieval systems~\cite{manning2009introduction}.
Therefore, query execution times are measured across all tasks detailed in
Section~\ref{sec:modelperfomance} to determine system suitability for various
operational demands.

\subsection{Baseline Models}
To establish a performance benchmark, two computationally efficient yet
well-regarded baselines were implemented: a TF-IDF embedding model~\cite{tfidf}
trained on the entire \emph{RecipeNLG} dataset after lemmatization, and a
SentenceBERT embedding model~\cite{sentence-bert}.
Both models generate embeddings for each recipe's full ingredient list.
For querying, the input query is embedded using the respective model, and the
entire dataset is ranked based on the cosine similarity between the query
embedding and the original recipe embeddings.
This ranked dataset is then used for evaluation.

\subsection{NER based database model}
This proposed model, specifically developed for this project, leverages a custom
NER model trained on recipe data.
It extracts ingredient list elements to construct a database, stored as a
Parquet~\footnote{\url{https://parquet.apache.org}} file and queried using
DuckDB~\cite{duckdb}.

\subsubsection{NER Tags}\label{sec:ner_tags}
The custom NER model was developed by refining an initial concept from a blog
post~\cite{ingNerwb} that used the \emph{TasteSET} dataset.
Finding the original \emph{TasteSET} tags insufficient, a new set of NER tags
was created and used to relabel the entire \emph{TasteSET} dataset, as well as
portions of \emph{RecipeNLG}.
These nine custom tags include: \emph{Quantity} (amount of unit); \emph{Unit}
(e.g., ``can'', ``lbs''); \emph{Food} (actual item, e.g., ``Tomato'',
``Chicken''); \emph{Variety} (food variant, e.g., ``Roma'', ``whole grain'');
\emph{Preparation} (food processing, e.g., ``finely sliced''); \emph{Alteration}
(reversible change, e.g., ``Frozen''); \emph{Brand} (e.g., ``Hershey's'');
\emph{Optional} (if ingredient is optional); and \emph{State} (user-unaffectable
condition, e.g., ``Large egg'', ``Fresh strawberries'').

While \emph{Quantity}, \emph{Unit}, and \emph{Food} tags largely align with
\emph{TasteSET}, others incorporate nuanced distinctions.
These tags were designed to accurately label properties relevant for querying,
with \emph{Food}, \emph{Variety}, and \emph{Preparation} being most central,
alongside \emph{Optional} and \emph{Brand} in some cases.
\emph{Alteration} and \emph{State} primarily ensure reversible processes are not
mislabeled as preparations, and irrelevant states are not conflated with
variants.
For instance, frozen raspberries should match recipes requiring raspberries, as
freezing is reversible.

This tagged data then trained a spaCy~\cite{spacy} NER model to extract these tags from text.

\subsubsection{Database creation}\label{sec:dbStructure}
To establish a queryable system leveraging unidirectional relationships, recipes
from our evaluation datasets are processed as follows: Each ingredient line is
first passed through our NER model.
Within each line, food tags are extracted, lemmatized, and multi-token foods are
appended with underscores (e.g., ``olive\_oil'')..
During this extraction, common items such as \emph{salt} and \emph{water},
assumed to be almost always present, are filtered out.
Preparations, Variants, and Brands are then extracted into maps, associating
each food item with a list of its respective lemmatized tokens (e.g.,
\{``tomato'': [``slice'', ``fine'']\}).
Only the first food item on a line is considered primary; any other food items
found on the same line are added to a list of alternate foods.
If an Optional entity is present, the entire line is marked as optional.
Finally, all collected items (primary food, alternate food, optional lists, and
the three maps) for each recipe are saved alongside original recipe information
as six newly added columns in a
Parquet~\footnote{\url{https://parquet.apache.org}} file, queried using
DuckDB~\cite{duckdb}.

\subsubsection{Querying}
Database queries are executed using DuckDB\@.
Query input first undergoes the same processing steps detailed in
Section~\ref{sec:dbStructure}.
These processed lists and maps then dynamically generate a DuckDB SQL query,
selecting only data where all specified food items exist in the database.
If preparations or variants are present, selection further requires that all
corresponding preparation tokens for each food in the query also exist in the
database.
For example, a query containing \{``tomato'': [``slice'']\} will match data with
\{``tomato'': [``slice'', ``fine'']\}, but not \{``tomato'': [``chop'']\}.
For testing, several query types were evaluated: combinations of food and
preparations, food with preparations and variants, and both of these repeated
with alternate food items joined.

Example queries are detailed in Appendix X.

\subsubsection{Ranking}
After querying, the returned data is an unranked subset requiring relevancy
sorting.
To rank this data, the \emph{BM25} algorithm, commonly used for search ranking,
is applied.
This involves creating a BM25 corpus from the processed food tokens (as
described in Section~\ref{sec:dbStructure}) and then ranking the results using
the food tokens extracted from the query.
While the model's output typically comprises the top recipes, the entire ranked
result is used for evaluation purposes.

\section{Results}
\section{Discussion}
\section{Conclusion}

\bibliography{references}

\appendix
\section{LLM Queries}\label{app:llm}
\begin{itemize}
\item \emph{``Summarize this ingredient list into a short query a person might type
    into google''} and then further processed with the follow up query:

    \emph{``could you expand upon the “google\_search\_query” field a bit, to make it “processed” as if a LLM had processed text as input to be used in a tool.
        The input would be the string Title, as well as a list of strings “Ingredients”.
        Could you from google\_search\_query, create another field,
        google\_search\_query\_processed, which is an object with the following structure:
\{
“title”: String (title of the recipe, if present, e.g.\ ``millionaire pie''),
“ingredients”: List[String (each ingredient and possible it’s preparations, seperated into a list of strings)]
\}''}
\item \emph{``Extract only the main food items and their preparations from
    this list''}
\item \emph{``I have the following ingredients and some of their
        preparations. List the most important ingredients and their
    preparations I should use to search a recipe with''} 
\end{itemize}



\end{document}

