\documentclass[11pt]{article}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}


\title{Unidirectional recipe search using custom NER models}
\date{}
\author{Anton Gefvert \\ Link√∂ping University}

\begin{document}
\maketitle
\begin{abstract}
    lorem ipsum
\end{abstract}

\section{Introduction}
\section{Introduction}
Recently, there has been a significant increase of applications leveraging LLM
agents to perform diverse tasks.
Many of these applications make use of Retrieval Augmented Generation (RAG),
which involves dynamically loading data into an LLM query without explicit user
command or visibility.
This enables LLMs to process more accurate and specific results.
In this work, this concept is applied specifically to recipes, with the aim of
creating a system to search through recipes using ingredients.
This system can then be leveraged by an LLM agent to perform queries such as
returning recipes or serving as a reference when creating new ones.

Ingredient lists for recipes possess characteristics that are particularly
relevant for developing a more nuanced search algorithm.
Among these, preparations and variants are of specific interest.
Preparations refer to how food is prepared (e.g., ``2 Tomatoes, \emph{chopped
into cubes}'' or ``3 red onions \emph{sliced thinly}''), while variants denote
specific ingredient variations (e.g., ``3 \emph{Roma} tomatoes'' or ``1 large
\emph{granny smith} apple'').

Typically, when searching for recipes, a user would most likely input general
terms such as ``tomatoes'' or ``apples'' to find recipes containing those
ingredients.
However, in scenarios where a user has, for instance, excess sliced red onions
and seeks a specific recipe for their utilization, or harbors a preference for
the tartness of Granny Smith apples and desires tailored inspiration, a more
sophisticated search capability becomes desirable.
Nevertheless, forcing the search query to be overly specific (e.g., for ``sliced
red onions'') is often counterproductive, as general ingredients (like red
onions) can be prepared as needed.
Similarly, for general ingredient searches (e.g., apples), recipes are likely to
remain satisfactory even if a specific variant (like Granny Smith) is not
precisely matched.

This work operationalizes this concept by utilizing ingredient-list-specific
Named Entity Recognition (NER) tags to establish uni-directional relationships
between ingredients and their associated preparations and variants.
This approach aims to demonstrate how fine-grained semantic understanding,
enabled by NER, can significantly enhance recipe retrieval systems, paving the
way for more intuitive and flexible culinary exploration tools.

\section{Theory}
To understand all parts of this report, some theory for things not discussed in
the TDDE16 course will be briefly explained.
\subsection{Top-K Accuracy}
Top-K Accuracy is an evaluation method for ranking algorithms.
Top-K Accuracy looks at, for all your ranking queries, in how many of them is the
target in the Top-K results.
The formula would be $\frac{\text{queries where target in top K}}{\text{number of
queries}}$, giving a fraction between $0$ and $1$\cite{topkacc}.
\subsection{Mean Reciprocal Rank}
Reciprocal Rank describes how well a relevant document is ranked in a list of
documents, quantified as a value between $0$ and $1$.
Reciprocal Rank is calculated using the formula $RR = 1/rank$ where $rank$ is the
rank in the list of documents, so if the relevant document is found at position
$5$, the reciprocal rank would be $1/5 = 0.2$.

Mean Reciprocal Rank (MRR) is the mean of the reciprocal rank over all queries,
calculated using the formula $MRR = \frac{1}{|Q|} \sum_{q=1}^Q{RR_q}$ where $Q$ is
the set of all queries and $RR_q$ is the reciprocal rank for query $q \in
Q$~\cite{mrr}.
\subsection{Best Match 25}
Best Match 25 (BM25) is a probabilistic ranking function widely used in
information retrieval to determine the relevance of documents to a given query.
It builds upon the TF-IDF (Term Frequency-Inverse Document Frequency) model by
introducing two key improvements: term frequency saturation (where repeated
occurrences of a term contribute less to relevance over time) and document
length normalization. These enhancements, controlled by adjustable parameters,
allow BM25 to more accurately rank documents than simpler TF-IDF
approaches~\cite{ogBM25, improvmenetsBM25}.

\section{Data}
In this work, two premade datasets are used,
\emph{RecipeNLG}\footnote{\url{https://www.kaggle.com/datasets/paultimothymooney/recipenlg}}
containing over 2 million recipes, as well as
\emph{TASTEset}~\cite{TASTEset}, a dataset of
700 recipes with ingredient-specific NER tags.

A third dataset, made by relabeling some of the NER tags in the TASTEset dataset,
as well as labeling 150 recipes from the RecipeNLG using the NER tags
described in Section~\ref{sec:ner_tags}, was created by manually labeling
data in Label Studio~\cite{LabelStudio}.
This dataset contains 850 recipes, and X tokens and Y NER tags.

\section{Method}
To create and evaluate this uni-directional relationship based retrieval system, 

\subsection{Evaluation Tasks}
To see how well the baselines and developed models perform the task of recipe
retrieval, as well as how well the unidirectional relationship filtering works,
a couple of evaluation tasks are defined.
Two types of task is used in this paper, model performance and query speed.
The first one to evaluate how well the models find relevant queryies.
The second one to evaluate the speed of which these queries are handled, to
gauge how suitable they are for different systems.
%For example, in a load heavy system that require low response times, model
%erformance might be compromisable as long as the query speed remains low.

\subsubsection{Model Performance}\label{sec:modelperfomance}
To evaluate the model performance, nine different evaluation methods were
created.
All the tasks are evaluated using \emph{Mean Reciprocal Rank (MRR)} \emph{Top-K
Accuracy}, to give a fair evaluation to search/ranking tasks.

The queries are separated into two groups, subset queries and adversarial
queries.

For the evaluation, there are two query dataset, one containing 50 recipes with
four different hand crafted queries, as well as one containing 1000 recipes with
four computer generated queries.
For each of these queries, they are tested against three dataset, the same 1000
recipes as the computer generated queries, as well as on datasets with 10k and
100k recipes, where all the recipes in the queries are present, all of which
are subsets of the \emph{RecipeNLG} datast.
Due to time and hardware resource constrains, the queries were not tested on the
entire dataset (running the processing on the entire dataset would take roughly
40 hours for just one of the models).

\paragraph{Subset Queries}
These queries take the input recipes, and transform them into queries that
contains as most as much information as the input, to see how well recipes are
found using queries that contain less information than the recipe itself.
These are tested using an identity query, three LLM generated queries, as well
as two hand crafted queries as described below.

\emph{Identity Query} --- This query simply queries the same recipe as the
original recipe in the datapoints.
This task is used as a sanity check, since we are querying the same data as we
have processed for our models, it should give close to a perfect result.

\emph{LLM Processed Queries} --- This task uses 1000 random
recipes, and for each recipe, inputs the ingredient list to a LLM with several
prompts to rephrase the input.
This is used to mimic a more human like input, while leveraging the power of
LLMs to quickly generate a larger dataset than a human annotated one.
For each recipe, the following prompts are used, and each promp is then used as
its own evaluation task:
\begin{itemize}
    \item \emph{``Summarize this ingredient list into a short query a person might type
        into google''} --- This returns a more human-esque way of inputting a
        recipe, rather than a list of ingredients and instructions.
        Since text like this query, is supposed to be input into an LLM that
        then uses a processed input to a function, it was also asked to process
        this sentence into a more structured format containing a title and a list
        of ingredients.
    \item \emph{``Extract only the main food items and their preparations from
        this list''} --- This returns a list that is more akin to the query a
        person (or program) is more likely to input to the model.
    \item \emph{``I have the following ingredients and some of their
            preparations. List the most important ingredients and their
        preparations I should use to search a recipe with''} --- This returns a
        list that is compromised only of the most important elements of the
        recipe.
\end{itemize}
Some examples of input and output is found in appendix X.

\emph{Human Processed Queries} --- This task uses 50 random recipes, that
are a subset of the previous task, and for each recipe, the author processed
these list into a more human like input.
For each list of ingredients, two different processing methods were used as
described below:
\begin{itemize}
    \item \textbf{A list of ingredients} --- for example ``Cucumber,
        sun-dried tomatoes, onion and ground beef'', mostly without preparations
        unless they are too core to the recipe, e.g.\ ``ground'' beef.
        This is to mimic a sample input of ``I have X food, what can I cook with it''
    \item \textbf{A list of prepared key ingredients} --- in this list the core
        and most unusal ingredients, with all of their preparations are compiled
        into a list; for example ``drained
        and quartered artichoke hearts; smoked turkey breast, cubed''.
        This is used to see if filtering on particular preparations can find
        very specific recipes.
\end{itemize}

\paragraph{Adversarial Queries} 
To evaluate how well the unidirectional filtering works, three adversarial query
tasks have been created.
These are to mimic queries that include more specific information than the
original recipe contains, meaning we do \emph{not} want to find the recipe in
question.
Because of this relationship, the metrics used for these tasks are inverted,
i.e.\ we use $1-MRR$ and $1-\text{Top-K Accuracy}$ for these queries.
This is tested using one automatically generated query, as well as two hand
crafted queries 

\emph{Automatically added preparation suffixes} --- This query takes a list of
common preparation suffixes, such as ``, chopped'' and ``, sliced'', and append
them to 40\% (minimum 1) of random ingredients in the ingredient list for each
recipe in the 1000 queries dataset.
This is to see how well the model filters away these recipes when adding extra
preparations that are not present in the original data.

\emph{Human annotated adversarial data} --- These two queries takes the
\emph{ingredient list} human annotated data, and adds variants or preparations
to roughly 40\% of the input that directly opposes the variants or preparations
used in the original data, e.g.\ if the original data has ``roma tomatoes,
chopped'', the new data might have ``cherry tomatoes, sliced''.
One dataset contains all the ingredients, while the other dataset contains a
subset of only the ingredients that have been modified.



\subsubsection{Query Speed}
Another important part of information retrieval systems is query speed,
therefore query speeds are also measured on all the tasks described in
Section~\ref{sec:modelperfomance}.

\subsection{Baselines models}
To get some idea of how performant the developed model is, two simple to
implement, but well regarded, baseline are implemented.
One TF-IDF embedding model~\cite{tfidf} trained on the entire \emph{RecipeNLG}
dataset, as well as a SentenceBERT~\cite{sentence-bert} embedding model.

Both of these models are used to create embeddings for each recipe on the entire
ingredient list.
When querying, the query is embedded using the same model, and the entire
dataset is then ranked based on the cosine similarity between the orignal
embeddings and the query embedding.
The final ranked dataset is then used for the evaluation.

\subsection{NER based database model}
This is proposed model specifically developed for this project.
The system as a whole, uses a custom trained NER model, trained on recipe data,
to extract the different elements of an ingredient list and then constructs a
database saved as a parquet~\footnote{\url{https://parquet.apache.org}} file,
which is then queried using DuckDB~\cite{duckdb}.

\subsubsection{NER Tags}\label{sec:ner_tags}
The original idea for the creation of a NER model for recipes came from a blog
post by~\citet{ingNerwb}, training a NER model on the \emph{TasteSET} dataset.
Looking at this dataset however, the tags were insufficient.
Thus this blog post and dataset were used as a base to create new NER tags, and
the entire \emph{TasteSET} dataset was relabled using these new tags:
\begin{enumerate}
    \item \emph{Quantity}. Quantity of unit
    \item \emph{Unit}. Unit, e.g.\ ``can'' or ``lbs''
    \item \emph{Food}. Actual food item, e.g.\ ``Tomato'' or ``Chicken''
    \item \emph{Variety}. Variety of food, e.g.\ ``Roma'' or ``whole grain''
    \item \emph{Preparation}. Preparation of food, e.g.\ ``finely sliced'' or
        ``boiled''
    \item \emph{Alteration}. Reversible change in food, e.g. ``Frozen'', ``Room
        temp''
    \item \emph{Brand}. Brand, e.g.\ ``Hershey's'' or ``Santa Maria''
    \item \emph{Optional}. If ingredient is optional, almost exclusively by
        seeing if the word ``optional'' is written on the line.
    \item \emph{State}. State which the user could not affect in any way, e.g.\
        ``Large'' (as in ``Large egg'') or ``Fresh'' (as in ``Fresh
        strawberries'')
\end{enumerate}

The \emph{Quantity}, \emph{Unit} and \emph{Food} tag are almost exclusively
directly taken from \emph{TasteSet}, whereas the rest have slighltly different
nuances.
These tags where created to correctly label the properties we are interested in
when querying, while adding some extra labels to avoid mislabeling of those
labels.
The labels that are the most interesting is \emph{Food}, \emph{Variety}, and
\emph{Preparation}, in some cases also \emph{Optional} and {Brand}.
The Labels \emph{Alteration} and \emph{State} is mainly used to make sure
processes that are actually reversible are not labeled as preparation, or
irrelevant states are not labeled as variants.
For example, if something is frozen, we can thaw it, therefore frozen
raspberries should be able to be used in a recipe with raspberries and vice
versa.

This tagged data is then used to train a spaCy~\cite{spacy} NER model, that
extracts these NER tags from texts.

\subsubsection{Database structure}\label{sec:dbStructure}
To leverage our unidirectional relationships and have a queryable system, the
recipes in our evaluation datasets are processed in the following way:
\begin{enumerate}
    \item The ingredient lists is passed through our NER model, one line at the
        time.
    \item Each line then has the following processing steps:
        \begin{enumerate}
            \item Extract any food tag found, lemmatize each token and append
                any multi-token food with underscores between (e.g.\
                ``olive\_oil'').
            \item Preparations, Variants, and Brands are extracted into maps,
                mapping food to a list containing each lemmatized token in the
                respective category \\(e.g.\ \{``tomato'':\ [``slice'',\
                ``fine'']\}).
            \item Only the first food item is considered as a food, if any other
                food items are found on the same line, these are added to
                another list of alternate foods.
            \item If there is any Optional entity in the line, the entire line
                is marked as optional.
        \end{enumerate}
    \item All the items processed in step 2 are then collected into lists of food, 
        alternate food, and optional, as well as maps for the three maps described in
        step 2.b.
\end{enumerate}
The above processes is run for each recipe in the data, and then saved to a
\emph{parquet} file, where the original recipe information, as well as the six newly
added columns is saved.

Note: In step 2.a, food items such as \emph{salt} and \emph{water} that are
assumed to almost always be present is filtered away.

\subsubsection{Querying}
To query the database, DuckDB queries are used.
First the data to be used as a query is run through the same processing steps as
explained in Section~\ref{sec:dbStructure}, these lists and maps are then used
to dynamically generate a DuckDB SQL query, only selecting data where all food
items in the query exists in the database.
If any preparations or variants are present, only data where the
preparations/variants found in the query is selected, by seeing if all
preparation token for each food that has a preparation in the query, also exists
in the database.
For example if our query contains the preparation \{``tomato'': [``slice'']\},
we will match data containing the preparation \{``tomato'': [``slice'',
``fine'']\}, but not \{``tomato'': [``chop'']\}.

For the testing, several different queries were tested, one with only food and
prepartions, one with food, preparation and variants, both of these were then
repeated while joining alternate food into the list of food.

Some example queries can be seen in appendix X.

\subsubsection{Ranking}
After running the query, the data returned is a subset of all the data matching
our query, but it is not ranked in any way.
Given that the system is supposed to return the most relevant recipes for our
query, the data needs to be sorted in relevancy.
To rank the data, the \emph{SM25} algorithm that is commonly used for search
ranking is used.
The way it is used is by creating a SM25 corpus on the processed food tokens
created as described in Section~\ref{sec:dbStructure}, and then using this
corpus to rank the results using the food tokens extracted before running the
query.

The top recipes of these ranked results is then the output of our model.
For evaluation purposes, the entire ranked result is used.


\iffalse{}
We need some way to perform ranking of the different models.
When using embeddings ranking gets trivial because we will use similarity
measures as our mean of searching either way.
A problem arise when we want to use normal database queries, as there is no way
to rank based of queries.
We can ofcourse sort by something like recipe rating, giving us the highest
rated recipes rather than the most relevant recipes (highest rated might be most
relevant).

An idea for how to rank the standar query version, is to use a score, similar to
tfidf for how many of the ingredients that is in the recipe, is in the query,
and negative score for any ingridient in the recipe that is not in the query.
For example if you have something like ``chopped tomato, parsley and tomato
paste'' as the query, you would add to the score for tomato, parsley and tomato
paste and any other ingredient would reduce the score.
The way something similar to tfidf can be used, is to decide how much the score
is affected by an ingredient, something like salt and pepper should not affect
the score too much, as these are prevalent in a lot of recipes and most likely
not something you would add to your query at all.
Using this, we could also assign scores to preparation steps, but this might be
redundant, as we will remove any recipe which does not contain the preparation
present in our query, and the point is you should be able to perform the
preparations if it is not already prepared.
In the above example, we would not find any recipes containing non-chopped
tomatoes, and if they are cooked as well, we can just cook our chopped tomatoes.

This does however raise the question, is a tfidf that also make use of our NER
tags to filter out results we do not want to see, sufficient? Maybe even more
efficient?
I also want to create a model using tfidf and filtering methods.
\fi

\section{Results}
\section{Discussion}
\section{Conclusion}

\bibliography{references}


\end{document}

