#### ---- FOOD ---- #####

# https://link-springer-com.e.bibl.liu.se/chapter/10.1007/978-3-030-30796-7_10
@InProceedings{foodKG,
    author="Haussmann, Steven
    and Seneviratne, Oshani
    and Chen, Yu
    and Ne'eman, Yarden
    and Codella, James
    and Chen, Ching-Hua
    and McGuinness, Deborah L.
    and Zaki, Mohammed J.",
    editor="Ghidini, Chiara
    and Hartig, Olaf
    and Maleshkova, Maria
    and Sv{\'a}tek, Vojt{\v{e}}ch
    and Cruz, Isabel
    and Hogan, Aidan
    and Song, Jie
    and Lefran{\c{c}}ois, Maxime
    and Gandon, Fabien",
    title="FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation",
    booktitle="The Semantic Web -- ISWC 2019",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    pages="146--162",
    abstract="The proliferation of recipes and other food information on the Web presents an opportunity for discovering and organizing diet-related knowledge into a knowledge graph. Currently, there are several ontologies related to food, but they are specialized in specific domains, e.g., from an agricultural, production, or specific health condition point-of-view. There is a lack of a unified knowledge graph that is oriented towards consumers who want to eat healthily, and who need an integrated food suggestion service that encompasses food and recipes that they encounter on a day-to-day basis, along with the provenance of the information they receive. Our resource contribution is a software toolkit that can be used to create a unified food knowledge graph that links the various silos related to food while preserving the provenance information. We describe the construction process of our knowledge graph, the plan for its maintenance, and how this knowledge graph has been utilized in several applications. These applications include a SPARQL-based service that lets a user determine what recipe to make based on ingredients at hand while taking constraints such as allergies into account, as well as a cognitive agent that can perform natural language question answering on the knowledge graph.",
    isbn="978-3-030-30796-7"
}


# https://www.scitepress.org/PublishedPapers/2021/102020/102020.pdf
@article{ExploitFoodEmb,
    title={Exploiting Food Embeddings for Ingredient Substitution.},
    author={Pellegrini, Chantal and {\"O}zsoy, Ege and Wintergerst, Monika and Groh, Georg},
    journal={HEALTHINF},
    volume={5},
    pages={67--77},
    year={2021}
}


# https://www.jmir.org/2021/8/e28229/
@Article{foodNerBERT,
    author="Stojanov, Riste
    and Popovski, Gorjan
    and Cenikj, Gjorgjina
    and Korou{\v{s}}i{\'{c}} Seljak, Barbara
    and Eftimov, Tome",
    title="A Fine-Tuned Bidirectional Encoder Representations From Transformers Model for Food Named-Entity Recognition: Algorithm Development and Validation",
    journal="J Med Internet Res",
    year="2021",
    month="Aug",
    day="9",
    volume="23",
    number="8",
    pages="e28229",
    keywords="food information extraction; named-entity recognition; fine-tuning BERT; semantic annotation; information extraction; BERT; bidirectional encoder representations from transformers; natural language processing; machine learning",
    abstract="Background: Recently, food science has been garnering a lot of attention. There are many open research questions on food interactions, as one of the main environmental factors, with other health-related entities such as diseases, treatments, and drugs. In the last 2 decades, a large amount of work has been done in natural language processing and machine learning to enable biomedical information extraction. However, machine learning in food science domains remains inadequately resourced, which brings to attention the problem of developing methods for food information extraction. There are only few food semantic resources and few rule-based methods for food information extraction, which often depend on some external resources. However, an annotated corpus with food entities along with their normalization was published in 2019 by using several food semantic resources. Objective: In this study, we investigated how the recently published bidirectional encoder representations from transformers (BERT) model, which provides state-of-the-art results in information extraction, can be fine-tuned for food information extraction. Methods: We introduce FoodNER, which is a collection of corpus-based food named-entity recognition methods. It consists of 15 different models obtained by fine-tuning 3 pretrained BERT models on 5 groups of semantic resources: food versus nonfood entity, 2 subsets of Hansard food semantic tags, FoodOn semantic tags, and Systematized Nomenclature of Medicine Clinical Terms food semantic tags. Results: All BERT models provided very promising results with 93.30{\%} to 94.31{\%} macro F1 scores in the task of distinguishing food versus nonfood entity, which represents the new state-of-the-art technology in food information extraction. Considering the tasks where semantic tags are predicted, all BERT models obtained very promising results once again, with their macro F1 scores ranging from 73.39{\%} to 78.96{\%}. Conclusions: FoodNER can be used to extract and annotate food entities in 5 different tasks: food versus nonfood entities and distinguishing food entities on the level of food groups by using the closest Hansard semantic tags, the parent Hansard semantic tags, the FoodOn semantic tags, or the Systematized Nomenclature of Medicine Clinical Terms semantic tags. ",
    issn="1438-8871",
    doi="10.2196/28229",
    url="https://www.jmir.org/2021/8/e28229",
    url="https://doi.org/10.2196/28229",
    url="http://www.ncbi.nlm.nih.gov/pubmed/34383671"
}

#https://arxiv.org/abs/2402.17447
@misc{deepLearninNERModelRecipes,
      title={Deep Learning Based Named Entity Recognition Models for Recipes}, 
      author={Mansi Goel and Ayush Agarwal and Shubham Agrawal and Janak Kapuriya and Akhil Vamshi Konam and Rishabh Gupta and Shrey Rastogi and Niharika and Ganesh Bagler},
      year={2024},
      eprint={2402.17447},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17447}, 
}


#### ---- ASSYMETRY ---- #####
# https://ojs.aaai.org/index.php/AAAI/article/view/10878
@inproceedings{scalableGraphEmb,
    title={Scalable graph embedding for asymmetric proximity},
    author={Zhou, Chang and Liu, Yuqiong and Liu, Xiaofei and Liu, Zhongyi and Gao, Jun},
    booktitle={Proceedings of the AAAI conference on artificial intelligence},
    volume={31},
    number={1},
    year={2017}
}

# https://dl-acm-org.e.bibl.liu.se/doi/abs/10.1145/2939672.2939751
@inproceedings{AsymTransGraphEmb,
    author = {Ou, Mingdong and Cui, Peng and Pei, Jian and Zhang, Ziwei and Zhu, Wenwu},
    title = {Asymmetric Transitivity Preserving Graph Embedding},
    year = {2016},
    isbn = {9781450342322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.e.bibl.liu.se/10.1145/2939672.2939751},
    doi = {10.1145/2939672.2939751},
    abstract = {Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.},
    booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {1105–1114},
    numpages = {10},
    keywords = {high-order proximity, graph embedding, directed graph, asymmetric transitivity},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}


#https://proceedings.neurips.cc/paper/2017/hash/59dfa2df42d9e3d41f5b02bfc32229dd-Abstract.html
@inproceedings{poincare,
    author = {Nickel, Maximillian and Kiela, Douwe},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Poincar\'{e} Embeddings for Learning Hierarchical Representations},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf},
    volume = {30},
    year = {2017}
}


#### ---- RANKING ---- #####
#https://dl-acm-org.e.bibl.liu.se/doi/abs/10.1145/2682862.2682863 
@inproceedings{improvmenetsBM25,
    author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
    title = {Improvements to BM25 and Language Models Examined},
    year = {2014},
    isbn = {9781450330008},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2682862.2682863},
    doi = {10.1145/2682862.2682863},
    abstract = {Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TF1°δ°p\texttimes{}ID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best over-all.},
    booktitle = {Proceedings of the 19th Australasian Document Computing Symposium},
    pages = {58–65},
    numpages = {8},
    keywords = {Document Retrieval, Procrastination, Relevance Ranking},
    location = {Melbourne, VIC, Australia},
    series = {ADCS '14}
}

#https://dl.acm.org/doi/abs/10.1145/361219.361220
@article{tfidf,
    author = {Salton, G. and Wong, A. and Yang, C. S.},
    title = {A vector space model for automatic indexing},
    year = {1975},
    issue_date = {Nov. 1975},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {18},
    number = {11},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/361219.361220},
    doi = {10.1145/361219.361220},
    abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
    journal = {Commun. ACM},
    month = nov,
    pages = {613–620},
    numpages = {8},
    keywords = {document space, content analysis, automatic information retrieval, automatic indexing}
}
