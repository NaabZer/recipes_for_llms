@INPROCEEDINGS{foodKG,
  ABSTRACT = {The proliferation of recipes and other food information on the Web presents an opportunity for discovering and organizing diet-related knowledge into a knowledge graph. Currently, there are several ontologies related to food, but they are specialized in specific domains, e.g., from an agricultural, production, or specific health condition point-of-view. There is a lack of a unified knowledge graph that is oriented towards consumers who want to eat healthily, and who need an integrated food suggestion service that encompasses food and recipes that they encounter on a day-to-day basis, along with the provenance of the information they receive. Our resource contribution is a software toolkit that can be used to create a unified food knowledge graph that links the various silos related to food while preserving the provenance information. We describe the construction process of our knowledge graph, the plan for its maintenance, and how this knowledge graph has been utilized in several applications. These applications include a SPARQL-based service that lets a user determine what recipe to make based on ingredients at hand while taking constraints such as allergies into account, as well as a cognitive agent that can perform natural language question answering on the knowledge graph.},
  AUTHOR = {Haussmann, Steven and Seneviratne, Oshani and Chen, Yu and Ne'eman, Yarden and Codella, James and Chen, Ching-Hua and McGuinness, Deborah L. and Zaki, Mohammed J.},
  EDITOR = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {The Semantic Web -- ISWC 2019},
  DATE = {2019},
  ISBN = {978-3-030-30796-7},
  PAGES = {146--162},
  TITLE = {FoodKG: A Semantics-Driven Knowledge Graph for Food Recommendation},
}

@ARTICLE{ExploitFoodEmb,
  AUTHOR = {Pellegrini, Chantal and Özsoy, Ege and Wintergerst, Monika and Groh, Georg},
  DATE = {2021},
  JOURNALTITLE = {HEALTHINF},
  PAGES = {67--77},
  TITLE = {Exploiting Food Embeddings for Ingredient Substitution.},
  VOLUME = {5},
}

@ARTICLE{foodNerBERT,
  ABSTRACT = {Background: Recently, food science has been garnering a lot of attention. There are many open research questions on food interactions, as one of the main environmental factors, with other health-related entities such as diseases, treatments, and drugs. In the last 2 decades, a large amount of work has been done in natural language processing and machine learning to enable biomedical information extraction. However, machine learning in food science domains remains inadequately resourced, which brings to attention the problem of developing methods for food information extraction. There are only few food semantic resources and few rule-based methods for food information extraction, which often depend on some external resources. However, an annotated corpus with food entities along with their normalization was published in 2019 by using several food semantic resources. Objective: In this study, we investigated how the recently published bidirectional encoder representations from transformers (BERT) model, which provides state-of-the-art results in information extraction, can be fine-tuned for food information extraction. Methods: We introduce FoodNER, which is a collection of corpus-based food named-entity recognition methods. It consists of 15 different models obtained by fine-tuning 3 pretrained BERT models on 5 groups of semantic resources: food versus nonfood entity, 2 subsets of Hansard food semantic tags, FoodOn semantic tags, and Systematized Nomenclature of Medicine Clinical Terms food semantic tags. Results: All BERT models provided very promising results with 93.30{\%} to 94.31{\%} macro F1 scores in the task of distinguishing food versus nonfood entity, which represents the new state-of-the-art technology in food information extraction. Considering the tasks where semantic tags are predicted, all BERT models obtained very promising results once again, with their macro F1 scores ranging from 73.39{\%} to 78.96{\%}. Conclusions: FoodNER can be used to extract and annotate food entities in 5 different tasks: food versus nonfood entities and distinguishing food entities on the level of food groups by using the closest Hansard semantic tags, the parent Hansard semantic tags, the FoodOn semantic tags, or the Systematized Nomenclature of Medicine Clinical Terms semantic tags.},
  AUTHOR = {Stojanov, Riste and Popovski, Gorjan and Cenikj, Gjorgjina and Koroušić Seljak, Barbara and Eftimov, Tome},
  URL = {http://www.ncbi.nlm.nih.gov/pubmed/34383671},
  DATE = {2021},
  DOI = {10.2196/28229},
  ISSN = {1438-8871},
  JOURNALTITLE = {J Med Internet Res},
  KEYWORDS = {food information extraction; named-entity recognition; fine-tuning BERT; semantic annotation; information extraction; BERT; bidirectional encoder representations from transformers; natural language processing; machine learning},
  NUMBER = {8},
  PAGES = {e28229},
  TITLE = {A Fine-Tuned Bidirectional Encoder Representations From Transformers Model for Food Named-Entity Recognition: Algorithm Development and Validation},
  VOLUME = {23},
}

@MISC{deepLearninNERModelRecipes,
  AUTHOR = {Goel, Mansi and Agarwal, Ayush and Agrawal, Shubham and Kapuriya, Janak and Konam, Akhil Vamshi and Gupta, Rishabh and Rastogi, Shrey and Niharika and Bagler, Ganesh},
  URL = {https://arxiv.org/abs/2402.17447},
  DATE = {2024},
  EPRINT = {2402.17447},
  EPRINTCLASS = {cs.CL},
  EPRINTTYPE = {arXiv},
  TITLE = {Deep Learning Based Named Entity Recognition Models for Recipes},
}

@INPROCEEDINGS{scalableGraphEmb,
  AUTHOR = {Zhou, Chang and Liu, Yuqiong and Liu, Xiaofei and Liu, Zhongyi and Gao, Jun},
  BOOKTITLE = {Proceedings of the AAAI conference on artificial intelligence},
  DATE = {2017},
  NUMBER = {1},
  TITLE = {Scalable graph embedding for asymmetric proximity},
  VOLUME = {31},
}

@INPROCEEDINGS{AsymTransGraphEmb,
  ABSTRACT = {Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.},
  AUTHOR = {Ou, Mingdong and Cui, Peng and Pei, Jian and Zhang, Ziwei and Zhu, Wenwu},
  LOCATION = {San Francisco, California, USA},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://doi-org.e.bibl.liu.se/10.1145/2939672.2939751},
  BOOKTITLE = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  DATE = {2016},
  DOI = {10.1145/2939672.2939751},
  ISBN = {9781450342322},
  KEYWORDS = {high-order proximity,graph embedding,directed graph,asymmetric transitivity},
  PAGES = {1105--1114},
  SERIES = {KDD '16},
  TITLE = {Asymmetric Transitivity Preserving Graph Embedding},
}

@INPROCEEDINGS{poincare,
  AUTHOR = {Nickel, Maximillian and Kiela, Douwe},
  EDITOR = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper_files/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2017},
  TITLE = {Poincaré Embeddings for Learning Hierarchical Representations},
  VOLUME = {30},
}

@ARTICLE{ogBM25,
  AUTHOR = {Robertson, Stephen E and Walker, Steve and Jones, Susan and Hancock-Beaulieu, Micheline M and Gatford, Mike and others},
  PUBLISHER = {National Instiute of Standards \& Technology},
  DATE = {1995},
  JOURNALTITLE = {Nist Special Publication Sp},
  PAGES = {109},
  TITLE = {Okapi at TREC-3},
  VOLUME = {109},
}

@INPROCEEDINGS{improvmenetsBM25,
  ABSTRACT = {Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TF1°δ°p\texttimes{}ID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best over-all.},
  AUTHOR = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
  LOCATION = {Melbourne, VIC, Australia},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://doi.org/10.1145/2682862.2682863},
  BOOKTITLE = {Proceedings of the 19th Australasian Document Computing Symposium},
  DATE = {2014},
  DOI = {10.1145/2682862.2682863},
  ISBN = {9781450330008},
  KEYWORDS = {Document Retrieval,Procrastination,Relevance Ranking},
  PAGES = {58--65},
  SERIES = {ADCS '14},
  TITLE = {Improvements to BM25 and Language Models Examined},
}

@ARTICLE{tfidf,
  ABSTRACT = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
  AUTHOR = {Salton, G. and Wong, A. and Yang, C. S.},
  LOCATION = {New York, NY, USA},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://doi.org/10.1145/361219.361220},
  DATE = {1975-11},
  DOI = {10.1145/361219.361220},
  ISSN = {0001-0782},
  JOURNALTITLE = {Commun. ACM},
  KEYWORDS = {document space,content analysis,automatic information retrieval,automatic indexing},
  NUMBER = {11},
  PAGES = {613--620},
  TITLE = {A vector space model for automatic indexing},
  VOLUME = {18},
}

@MISC{bm25s,
  AUTHOR = {Lù, Xing Han},
  URL = {https://arxiv.org/abs/2407.03618},
  DATE = {2024},
  EPRINT = {2407.03618},
  EPRINTCLASS = {cs.IR},
  EPRINTTYPE = {arXiv},
  TITLE = {BM25S: Orders of magnitude faster lexical search via eager sparse scoring},
}

@MISC{LabelStudio,
  AUTHOR = {Tkachenko, Maxim and Malyuk, Mikhail and Holmanyuk, Andrey and Liubimov, Nikolai},
  URL = {https://github.com/HumanSignal/label-studio},
  DATE = {2025},
  NOTE = {Open source software available from https://github.com/HumanSignal/label-studio},
  TITLE = {{Label Studio}: Data labeling software},
}

